<!DOCTYPE html>
<html lang="en-GB">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Bath Visual Computing AI & ML Reading Group">

  <title>AI Reading Group</title>
  <link rel="stylesheet" href="style.css">
  <!-- <link rel="shortcut icon" type="image/png" href="bvc-icon.png" /> -->
</head>

<body>

<div class="container">
  <!-- <img src="bvc-logo.png" width="400" alt="Logo for Bath AI & ML Group" style="display:block; margin: 0.5em auto;" /> -->
  <h4 class="heading center_text">Reading Group and Research Seminar</h4>
  <hr>

  <p class="location_time center_text">
    <b>Thursdays from 13:15 to 14:05.  Feel free to bring your lunch!</b>
  </p>

  <p>
    This page contains information and the schedule for the Bath AI & ML reading group and research seminar.
    Every other week one student or member of staff will present a paper.   The paper for the next session will be mailed out in advance.  It would be great if attendees 
	  could have read the paper beforehand, but the speaker will explain the paper so this is not strictly necessary.   This is intended to be a friendly and supportive environment where 
    we collectively explore a new idea.   Do what you can to make it comfortable for the speaker, especially if they are junior. </p>

   <p>A list of forthcoming speakers is presented at the bottom of this page.  The order is randomized.  We understand that there are conference deadlines and that people go on vacations. 
    If you can't make your week, then it is up to you to swap with someone else further down the list.  </p> 

   <p> Why is attending this important?   All great academic gropus are more than the sum of their parts.   This is a venue where you can meet and discuss ideas with people from your own and other groups.  
	  Throughout the year you will be exposed to 26 new ideas.  Many of these will be interesting but not directly influence your work, but every now and then you will get an idea that you can apply to your own research.
	  In addition, then ensuing discussions will mean that you become familiar with everyone else's expertise.  Maybe knowledge that you have could be useful to someone else writing a NeurIPS paper.  Boom! Co-authorship.  
	  More than the sum of our parts.
  </p>

  <h5>Reading Group</h5>

  <p>
    <u>Preparation:</u>
    The organizer (currently Sophia Jones) will contact you two weeks in advance to ask for the paper that you will present.   You can choose whatever you want, but please choose something general/important enough that it will be of interset to most of the group.  
    If you don't have any ideas, a suggests list of papers is kept <a href="https://docs.google.com/document/d/1dHP-4nOF-qH9EOdxSb1AQa3nSe0ixJdKfjn_ZN8MAmI/edit#heading=h.vvbkm4wwzx5j"> here</a>
  Feel free to add any interesting looking papers to the list! 

   The room is booked for a full hour, but it's assumed that most presentations will take about 20 minutes, leaving plenty of time for discussion.   Slides are encouraged, but working through the PDF of the paper is also acceptable.

  </p>
  <h4 class="center_text">Schedule for 2022</h4>
  <hr>

  <div class="table-wrapper">
    <table id="schedule" class="table table-hover table-striped">
      <thead>
        <tr>
          <th scope="col" width="10%">Date</th>
          <th scope="col" width="45%">Reading&nbsp;Group</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>14&nbsp;Dec &nbsp;2023</td>
          <td>Prof Simon Prince:<br><a href="https://arxiv.org/abs/2101.12176"><b>On the Origin of Implicit Regularization in Stochastic Gradient Descent</b></a> Samuel L. Smith, Benoit Dherin, David G. T. Barrett, Soham De, ICLR 2021</td>
        </tr>
	<tr>
          <td>11&nbsp;Jan &nbsp;2024</td>
          <td>Josh Evans:<br><a href="https://arxiv.org/abs/2101.12176"><b>Creating Multi-Level Skill Hierarchies in Reinforcement Learning</b></a> Joshua Benjamin Evans and {\"O}zg{\"u}r {\c{S}}im{\c{s}}ek}, NeurIPS 2023</td>
        </tr>
	<tr>
          <td>25&nbsp;Jan &nbsp;2024</td>
          <td>Jundan Luo:<br><a href="https://arxiv.org/abs/2101.12176"><b>On the Origin of Implicit Regularization in Stochastic Gradient Descent</b></a> Samuel L. Smith, Benoit Dherin, David G. T. Barrett, Soham De, ICLR 2021</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<h5>Future speakers</h5>

<ol>
  <li>Stevie Wonder</li>
  <li>Rishi Sunak</li>
  <li>William Shatner</li>
</ol>  


</body>
</html>
